{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "604a83e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e81dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ ØµÙØ­Ø© Ø§Ù„Ø¨Ø±ÙˆÙØ§ÙŠÙ„..\n",
      "â³ Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©...\n",
      "â³ Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­!\n",
      "ğŸ”„ Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª...\n",
      "âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­!\n",
      "ğŸ”„ Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª...\n"
     ]
    }
   ],
   "source": [
    "# Initialize the driver\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.instagram.com/\")\n",
    "\n",
    "# Login\n",
    "time.sleep(5)\n",
    "username = driver.find_element(By.CSS_SELECTOR, \"input[name='username']\")\n",
    "password = driver.find_element(By.CSS_SELECTOR, \"input[name='password']\")\n",
    "username.clear()\n",
    "password.clear()\n",
    "username.send_keys(\"        \")\n",
    "password.send_keys(\"        \")\n",
    "login = driver.find_element(By.CSS_SELECTOR, \"button[type='submit']\").click()\n",
    "time.sleep(5)\n",
    "\n",
    "# Go directly to the profile page\n",
    "print(\"ğŸ“ Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ Ø¥Ù„Ù‰ ØµÙØ­Ø© Ø§Ù„Ø¨Ø±ÙˆÙØ§ÙŠÙ„..\")\n",
    "time.sleep(3)\n",
    "driver.get(\"https://www.instagram.com/l3gq/\")\n",
    "print(\"â³ Ø§Ù†ØªØ¸Ø§Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø©...\")\n",
    "time.sleep(3)  # Ø§Ù†ØªØ¸Ø§Ø± 3 Ø«ÙˆØ§Ù†ÙŠ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ø§Ù„ÙƒØ§Ù…Ù„\n",
    "print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ Ø§Ù„ØµÙØ­Ø© Ø¨Ù†Ø¬Ø§Ø­!\")\n",
    "print(\"ğŸ”„ Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a6d76ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… ØªÙ… Ø¬Ù…Ø¹ 36 Ø±Ø§Ø¨Ø· Ø¨ÙˆØ³Øª.\n",
      "ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª ÙÙŠ: collected_post_links_20251115_005515.csv\n",
      "âœ… Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ 36 Ø±Ø§Ø¨Ø·\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Ø¬Ù…Ø¹ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª Ù…Ø¹ Ø³ÙƒØ±ÙˆÙ„ ---\n",
    "all_posts = set()\n",
    "no_change = 0\n",
    "last_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "\n",
    "while no_change < 3:\n",
    "    links = driver.find_elements(By.TAG_NAME, 'a')\n",
    "    for link in links:\n",
    "        href = link.get_attribute('href')\n",
    "        if href and ('/p/' in href or '/reel/' in href):\n",
    "            all_posts.add(href)\n",
    "    time.sleep(2)\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    time.sleep(4)\n",
    "\n",
    "    new_height = driver.execute_script(\"return document.body.scrollHeight\")\n",
    "    if new_height == last_height:\n",
    "        no_change += 1\n",
    "    else:\n",
    "        no_change = 0\n",
    "    last_height = new_height\n",
    "\n",
    "posts = list(all_posts)\n",
    "print(f\"âœ… ØªÙ… Ø¬Ù…Ø¹ {len(posts)} Ø±Ø§Ø¨Ø· Ø¨ÙˆØ³Øª.\")\n",
    "\n",
    "# --- Ø­ÙØ¸ Ø§Ù„Ø±ÙˆØ§Ø¨Ø· ÙÙŠ Ù…Ù„Ù CSV ---\n",
    "df_posts = pd.DataFrame({\n",
    "    'Post Number': range(1, len(posts) + 1),\n",
    "    'Post Link': posts\n",
    "})\n",
    "\n",
    "posts_filename = f\"collected_post_links_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "df_posts.to_csv(posts_filename, index=False, encoding='utf-8-sig')\n",
    "print(f\"ğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø±ÙˆØ§Ø¨Ø· Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª ÙÙŠ: {posts_filename}\")\n",
    "print(f\"âœ… Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(posts)} Ø±Ø§Ø¨Ø·\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97031d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: parse compact like strings (\"1.2K\", \"3M\", \"1,234\") and robustly extract likes from the post DOM\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException, TimeoutException\n",
    "\n",
    "def _parse_compact_number(s):\n",
    "    \"\"\"Convert strings like '1,234', '1.2K', '3M' to int. Returns None if can't parse.\"\"\"\n",
    "    if not s:\n",
    "        return None\n",
    "    s = s.strip()\n",
    "    # try direct integer with commas\n",
    "    try:\n",
    "        return int(s.replace(\",\", \"\"))\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    # compact form like 1.2K, 3M, 800k\n",
    "    m = re.match(r\"^([\\d\\.]+)\\s*([kKmM])$\", s)\n",
    "    if m:\n",
    "        val = float(m.group(1))\n",
    "        suf = m.group(2).lower()\n",
    "        if suf == 'k':\n",
    "            return int(val * 1_000)\n",
    "        if suf == 'm':\n",
    "            return int(val * 1_000_000)\n",
    "    # fallback: extract first plain number with commas or dots\n",
    "    m2 = re.search(r\"([\\d,]+)(?:\\.|,\\d+)?\", s)\n",
    "    if m2:\n",
    "        try:\n",
    "            return int(m2.group(1).replace(',', ''))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "\n",
    "def get_likes_count(driver, root=None, timeout=2, verbose=False):\n",
    "    \"\"\"\n",
    "    Robustly extract the likes count from a post page.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        root: optional WebElement to scope the search (e.g., article element). If None, search the whole page.\n",
    "        timeout: not used for explicit wait here but kept for compatibility.\n",
    "        verbose: if True prints debug messages.\n",
    "\n",
    "    Returns:\n",
    "        int likes_count or None if not found/parsable.\n",
    "    \"\"\"\n",
    "    search_root = root if root is not None else driver\n",
    "\n",
    "    strategies = []\n",
    "\n",
    "    # 1) anchor to liked_by (most reliable in the snippet you provided)\n",
    "    strategies.append((By.XPATH, \".//a[contains(@href, '/liked_by')]\") if root is not None else (By.XPATH, \"//a[contains(@href, '/liked_by')]\") )\n",
    "\n",
    "    # 2) common spans/buttons inside section that show like counts\n",
    "    strategies.append((By.XPATH, \".//section//span[contains(., 'likes') or contains(., 'like')]\") if root is not None else (By.XPATH, \"//section//span[contains(., 'likes') or contains(., 'like')]\") )\n",
    "\n",
    "    # 3) any span/button with numeric text near role='button'\n",
    "    strategies.append((By.XPATH, \".//span[@role='button' and string-length(normalize-space(.))>0]\") if root is not None else (By.XPATH, \"//span[@role='button' and string-length(normalize-space(.))>0]\") )\n",
    "\n",
    "    # 4) fallback: any element that contains word 'likes' and a number\n",
    "    strategies.append((By.XPATH, \".//*[contains(translate(., 'LIKES', 'likes'), 'likes')]\") if root is not None else (By.XPATH, \"//*[contains(translate(., 'LIKES', 'likes'), 'likes')]\") )\n",
    "\n",
    "    for by, expr in strategies:\n",
    "        try:\n",
    "            if verbose:\n",
    "                print('Trying strategy:', by, expr)\n",
    "            elems = search_root.find_elements(by, expr)\n",
    "            for el in elems:\n",
    "                # prefer visible text\n",
    "                text = ''\n",
    "                try:\n",
    "                    text = el.text.strip()\n",
    "                except Exception:\n",
    "                    try:\n",
    "                        text = el.get_attribute('innerText') or el.get_attribute('textContent') or ''\n",
    "                    except Exception:\n",
    "                        text = ''\n",
    "\n",
    "                if not text:\n",
    "                    # try attributes that sometimes contain counts\n",
    "                    for attr in ('aria-label', 'title', 'data-count'):\n",
    "                        try:\n",
    "                            val = el.get_attribute(attr)\n",
    "                            if val:\n",
    "                                text = val\n",
    "                                break\n",
    "                        except Exception:\n",
    "                            continue\n",
    "\n",
    "                if not text:\n",
    "                    # try nested span\n",
    "                    try:\n",
    "                        sp = el.find_element(By.XPATH, \".//span\")\n",
    "                        text = sp.text.strip() if sp.text else (sp.get_attribute('innerText') or '')\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                if not text:\n",
    "                    continue\n",
    "\n",
    "                # extract digits or compact formats\n",
    "                # first try to find a number next to 'like' word\n",
    "                # examples: '32 likes', '1,234 likes', '1.2K likes'\n",
    "                m = re.search(r\"([\\d\\.,]+\\s*[kKmM]?)\", text)\n",
    "                if m:\n",
    "                    candidate = m.group(1).strip()\n",
    "                    parsed = _parse_compact_number(candidate)\n",
    "                    if parsed is not None:\n",
    "                        if verbose:\n",
    "                            print('Parsed', candidate, '->', parsed)\n",
    "                        return parsed\n",
    "\n",
    "        except Exception as e:\n",
    "            if verbose:\n",
    "                print('strategy error:', e)\n",
    "            continue\n",
    "\n",
    "    # final fallback: search whole page text for pattern like '123 likes'\n",
    "    try:\n",
    "        page_text = driver.page_source\n",
    "        m = re.search(r\"([\\d\\.,]+\\s*[kKmM]?)\\s*(?:likes|like|Ø¥Ø¹Ø¬Ø§Ø¨Ø§Øª|Ø¥Ø¹Ø¬Ø§Ø¨)\", page_text)\n",
    "        if m:\n",
    "            parsed = _parse_compact_number(m.group(1))\n",
    "            if parsed is not None:\n",
    "                return parsed\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "    if verbose:\n",
    "        print('Likes count not found')\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1e5a005c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‡Ø´ØªØ§Ø¬Ø§Øª ---\n",
    "def extract_hashtags(caption_text):\n",
    "    \"\"\"\n",
    "    Ø¯Ø§Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‡Ø´ØªØ§Ø¬Ø§Øª Ù…Ù† Ù†Øµ Ø§Ù„ÙƒØ§Ø¨Ø´Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… regex.\n",
    "\n",
    "    Args:\n",
    "        caption_text: Ù†Øµ Ø§Ù„ÙƒØ§Ø¨Ø´Ù†.\n",
    "\n",
    "    Returns:\n",
    "        Ø³Ù„Ø³Ù„Ø© Ù†ØµÙŠØ© Ù…ÙØµÙˆÙ„Ø© Ø¨Ù…Ø³Ø§ÙØ§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø´ØªØ§Ø¬Ø§Øª ÙÙ‚Ø·.\n",
    "    \"\"\"\n",
    "    return ' '.join(re.findall(r'#\\w+', caption_text)) if caption_text else ''\n",
    "\n",
    "\n",
    "# --- Ø§Ù„Ø¯Ø§Ù„Ø© Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù†Ø´Ù† ---\n",
    "def extract_captions(caption_text):\n",
    "    \"\"\"\n",
    "    Ø¯Ø§Ù„Ø© Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‡Ø´ØªØ§Ø¬Ø§Øª Ù…Ù† Ù†Øµ Ø§Ù„ÙƒØ§Ø¨Ø´Ù† Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… regex.\n",
    "\n",
    "    Args:\n",
    "        caption_text: Ù†Øµ Ø§Ù„ÙƒØ§Ø¨Ø´Ù†.\n",
    "\n",
    "    Returns:\n",
    "        Ø³Ù„Ø³Ù„Ø© Ù†ØµÙŠØ© Ù…ÙØµÙˆÙ„Ø© Ø¨Ù…Ø³Ø§ÙØ§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø´ØªØ§Ø¬Ø§Øª ÙÙ‚Ø·.\n",
    "    \"\"\"\n",
    "    return ' '.join(re.findall(r'@\\w+', caption_text)) if caption_text else ''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721ba052",
   "metadata": {},
   "outputs": [],
   "source": [
    "if posts:\n",
    "    # Ø¥Ù†Ø´Ø§Ø¡ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ\n",
    "    all_posts_data = {\n",
    "        \"Post Number\": [],\n",
    "        \"Post Link\": [],\n",
    "        \"Media Type\": [],\n",
    "        \"Post Date\": [],\n",
    "        \"Likes Count\": [],\n",
    "        \"Caption\": [],\n",
    "        \"Hashtags\": [],\n",
    "        \"Comments\": [],\n",
    "        \"Comment Count\": []\n",
    "    }\n",
    "\n",
    "    # Loop Ø¹Ù„Ù‰ ÙƒÙ„ Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª\n",
    "    for post_index, post_url in enumerate(posts, start=1):\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"ğŸ”— ÙØªØ­ Ø§Ù„Ø¨ÙˆØ³Øª Ø±Ù‚Ù… {post_index} Ù…Ù† {len(posts)}: {post_url}\")\n",
    "        print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "        driver.get(post_url)\n",
    "        time.sleep(3)\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.XPATH, \"//article | //div[contains(@class, 'x1n2onr6')]\")))\n",
    "            print(\"âœ… ØªÙ… ØªØ­Ù…ÙŠÙ„ ØµÙØ­Ø© Ø§Ù„Ø¨ÙˆØ³Øª.\")\n",
    "        except TimeoutException:\n",
    "            print(\"âš ï¸ Ø±Ø¨Ù…Ø§ ØªØ£Ø®Ø± ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙˆØ³ØªØŒ Ø§Ø³ØªÙ…Ø±Ø§Ø± Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø©...\")\n",
    "\n",
    "        # ============================================\n",
    "        # ğŸ“Š Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙˆØ³Øª Ø§Ù„Ù…ÙØªÙˆØ­\n",
    "        # ============================================\n",
    "\n",
    "        # Ø¥Ø¶Ø§ÙØ© Ø±Ù‚Ù… Ø§Ù„Ø¨ÙˆØ³Øª ÙˆØ§Ù„Ø±Ø§Ø¨Ø·\n",
    "        all_posts_data[\"Post Number\"].append(post_index)\n",
    "        all_posts_data[\"Post Link\"].append(post_url)\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        # --- 1. Post Date ---\n",
    "        try:\n",
    "            time_element = wait.until(EC.presence_of_element_located((By.XPATH, \"//time[@datetime]\")))\n",
    "            post_date = time_element.get_attribute('datetime')\n",
    "            all_posts_data[\"Post Date\"].append(post_date)\n",
    "            print(f\"ğŸ“… Post Date: {post_date}\")\n",
    "        except TimeoutException:\n",
    "            print(\"âŒ Post Date: Not Found\")\n",
    "            all_posts_data[\"Post Date\"].append(\"N/A\")\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        # --- 2. Media Type ---\n",
    "        try:\n",
    "            video_element = driver.find_elements(By.TAG_NAME, \"video\")\n",
    "            if video_element:\n",
    "                media_type = \"Video\"\n",
    "            else:\n",
    "                img_element = driver.find_elements(By.TAG_NAME, \"img\")\n",
    "                if img_element:\n",
    "                    media_type = \"Image\"\n",
    "                else:\n",
    "                    media_type = \"Unknown\"\n",
    "            all_posts_data[\"Media Type\"].append(media_type)\n",
    "            print(f\"ğŸ–¼ï¸ Media Type: {media_type}\")\n",
    "        except NoSuchElementException:\n",
    "            print(\"âŒ Media Type: Not Found\")\n",
    "            all_posts_data[\"Media Type\"].append(\"N/A\")\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        # --- 3. Caption & Hashtags (Ù…Ø­Ø¯Ø« Ù„Ø§Ù„ØªÙ‚Ø§Ø· Ø§Ù„ÙƒØ§Ø¨Ø´Ù† Ø§Ù„ÙƒØ§Ù…Ù„) ---\n",
    "        caption = ''\n",
    "        captions = ''\n",
    "        hashtags = ''\n",
    "\n",
    "        try:\n",
    "            # ğŸ¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰: Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ù…Ø¨Ø§Ø´Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… XPath Ø¯Ù‚ÙŠÙ‚\n",
    "            # Ù†Ø¨Ø­Ø« Ø¹Ù† span ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù†Øµ Ø¹Ø±Ø¨ÙŠ Ø£Ùˆ Ù‡Ø§Ø´ØªØ§Ù‚ Ø¯Ø§Ø®Ù„ Ù‡ÙŠÙƒÙ„ Ø´Ø§Ø¦Ø¹ ÙÙŠ Ù…Ù†Ø´ÙˆØ±Ø§Øª Ø¥Ù†Ø³ØªØºØ±Ø§Ù…\n",
    "            caption_element = driver.find_element(\n",
    "                By.XPATH,\n",
    "                \"//div[contains(@class, 'x9f619') and contains(@class, 'xjbqb8w')]//span[contains(text(), '#') or contains(text(), '@')]\"\n",
    "            )\n",
    "            caption = caption_element.text.strip()\n",
    "            print(f\"âœ… Ø§Ù„ÙƒØ§Ø¨Ø´Ù† (Ø¨Ø§Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø§Ù„Ù…Ø¨Ø§Ø´Ø±): {caption[:200]}...\")\n",
    "\n",
    "        except Exception as e1:\n",
    "            print(f\"âš ï¸ ÙØ´Ù„ Ø§Ù„Ø§Ø³ØªÙ‡Ø¯Ø§Ù Ø§Ù„Ù…Ø¨Ø§Ø´Ø±: {e1}\")\n",
    "\n",
    "            try:\n",
    "                # ğŸ”„ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø«Ø§Ù†ÙŠØ©: Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø§Ù… Ø¹Ù† span Ø·ÙˆÙŠÙ„ Ø£Ùˆ ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‡Ø§Ø´ØªØ§Ù‚\n",
    "                all_spans = driver.find_elements(By.TAG_NAME, \"span\")\n",
    "                full_caption = ''\n",
    "\n",
    "                # Ø£ÙˆÙ„Ù‹Ø§: Ø§Ø¨Ø­Ø« Ø¹Ù† span ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ù‡Ø§Ø´ØªØ§Ù‚ (Ø§Ù„Ø£ÙƒØ«Ø± Ø¯Ù‚Ø©)\n",
    "                for span in all_spans:\n",
    "                    text = span.text.strip()\n",
    "                    if text and '#' in text:\n",
    "                        full_caption = text\n",
    "                        break\n",
    "\n",
    "                # Ø«Ø§Ù†ÙŠÙ‹Ø§: Ø¥Ø°Ø§ Ù„Ù… ÙŠÙØ¹Ø«Ø±ØŒ Ø§Ø¨Ø­Ø« Ø¹Ù† span Ø·ÙˆÙŠÙ„ (> 30 Ø­Ø±ÙÙ‹Ø§)\n",
    "                if not full_caption:\n",
    "                    for span in all_spans:\n",
    "                        text = span.text.strip()\n",
    "                        if text and len(text) > 30:\n",
    "                            full_caption = text\n",
    "                            break\n",
    "\n",
    "                # Ø«Ø§Ù„Ø«Ù‹Ø§: Ø¥Ø°Ø§ ÙØ´Ù„ ÙƒÙ„ Ø´ÙŠØ¡ØŒ Ø®Ø° Ø¢Ø®Ø± span ØºÙŠØ± ÙØ§Ø±Øº Ù…Ø¹Ù‚ÙˆÙ„ Ø§Ù„Ø·ÙˆÙ„\n",
    "                if not full_caption:\n",
    "                    for span in reversed(all_spans):\n",
    "                        text = span.text.strip()\n",
    "                        if text and len(text) > 10:\n",
    "                            full_caption = text\n",
    "                            break\n",
    "\n",
    "                caption = full_caption\n",
    "                print(f\"âœ… Ø§Ù„ÙƒØ§Ø¨Ø´Ù† (Ø¨Ø§Ù„Ø¨Ø­Ø« Ø§Ù„Ø¹Ø§Ù…): {caption[:200]}...\" if caption else \"âŒ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ ÙƒØ§Ø¨Ø´Ù†.\")\n",
    "\n",
    "            except Exception as e2:\n",
    "                print(f\"âŒ ÙØ´Ù„ Ø¬Ù…ÙŠØ¹ Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø¬Ù„Ø¨ Ø§Ù„ÙƒØ§Ø¨Ø´Ù†: {e2}\")\n",
    "                caption = ''\n",
    "\n",
    "        # --- Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù‡Ø´ØªØ§Ø¬Ø§Øª ÙˆØ§Ù„Ù…Ù†Ø´Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© ---\n",
    "        hashtags = extract_hashtags(caption)\n",
    "        captions = extract_captions(caption)\n",
    "        print(f\"âœ… Ø§Ù„Ù‡Ø´ØªØ§Ø¬Ø§Øª: {hashtags if hashtags else 'N/A'}\")\n",
    "        print(f\"âœ… Ø§Ù„Ù…Ù†Ø´Ù†Ø§Øª: {captions if captions else 'N/A'}\")\n",
    "\n",
    "        # --- Ø­ÙØ¸ ÙÙŠ Ù‡ÙŠÙƒÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ---\n",
    "        all_posts_data[\"Caption\"].append(captions if captions else \"No captions\")\n",
    "        all_posts_data[\"Hashtags\"].append(hashtags if hashtags else \"No hashtags\")\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        # --- 4. Likes Count (Ù…Ø­Ø¯Ø«) ---\n",
    "        try:\n",
    "            # use the robust helper we added earlier\n",
    "            likes_count = get_likes_count(driver, root=None, timeout=2, verbose=False)\n",
    "            likes_count = str(likes_count) if likes_count is not None else \"N/A\"\n",
    "            all_posts_data[\"Likes Count\"].append(likes_count)\n",
    "            print(f\"â¤ï¸ Likes Count: {likes_count}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Likes Count Error: {str(e)}\")\n",
    "            all_posts_data[\"Likes Count\"].append(\"N/A\")\n",
    "\n",
    "        # ======================================================================================================\n",
    "\n",
    "        # --- 5. Ø¬Ù…Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª (Comments) Ù…Ø¹ Ø§Ù„Ø³ÙƒØ±ÙˆÙ„ Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„ÙƒÙ„ ---\n",
    "        comments_set = set()\n",
    "        num_comments = 0\n",
    "\n",
    "        try:\n",
    "            print(\"ğŸ”„ Ø¨Ø¯Ø¡ Ø¬Ù…Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù…Ø¹ Ø§Ù„Ø³ÙƒØ±ÙˆÙ„...\")\n",
    "\n",
    "            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¶ØºØ· Ø¹Ù„Ù‰ \"Ø¹Ø±Ø¶ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª\"\n",
    "            try:\n",
    "                view_all = WebDriverWait(driver, 3).until(\n",
    "                    EC.element_to_be_clickable((\n",
    "                        By.XPATH,\n",
    "                        \"//button[contains(text(), 'View all') or contains(text(), 'View all 1 replies') or contains(text(), 'View all 2 replies')]\"\n",
    "                    ))\n",
    "                )\n",
    "                view_all.click()\n",
    "                time.sleep(2)\n",
    "                print(\"âœ… ØªÙ… Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ 'Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²ÙŠØ¯'\")\n",
    "            except:\n",
    "                print(\"â„¹ï¸ Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø²Ø± 'Ø¹Ø±Ø¶ Ø§Ù„Ù…Ø²ÙŠØ¯'\")\n",
    "                pass\n",
    "\n",
    "            # Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø§ÙˆÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª (Ø¥Ù† ÙˆÙØ¬Ø¯Øª)\n",
    "            comments_container = None\n",
    "\n",
    "            # Ù…Ø­Ø§ÙˆÙ„Ø© Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø§ÙˆÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„ÙØ¹Ù„ÙŠØ© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ù€ class Ø§Ù„Ù…Ù…ÙŠØ²\n",
    "            try:\n",
    "                comments_container = WebDriverWait(driver, 5).until(\n",
    "                    EC.presence_of_element_located((\n",
    "                        By.XPATH,\n",
    "                        \"//div[contains(@class, 'x5yr21d') and contains(@class, 'xw2csxc') and contains(@class, 'x1odjw0f')]\"\n",
    "                    ))\n",
    "                )\n",
    "                print(\"âœ… ØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø§ÙˆÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©\")\n",
    "            except TimeoutException:\n",
    "                print(\"âš ï¸ Ù„Ù… ÙŠØªÙ… Ø§Ù„Ø¹Ø«ÙˆØ± Ø¹Ù„Ù‰ Ø­Ø§ÙˆÙŠØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª\")\n",
    "                comments_container = None\n",
    "\n",
    "            # --- Ø§Ù„Ø³ÙƒØ±ÙˆÙ„ ÙˆØ¬Ù…Ø¹ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª Ø¨Ø´ÙƒÙ„ ØªØ¯Ø±ÙŠØ¬ÙŠ ---\n",
    "            last_height = 0\n",
    "            same_height = 0\n",
    "            max_same = 5  # Ø¹Ø¯Ø¯ Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø§Øª Ø§Ù„Ù…ØªØªØ§Ù„ÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„ØªÙˆÙ‚Ù\n",
    "            scroll_count = 0\n",
    "\n",
    "            while same_height < max_same:\n",
    "                scroll_count += 1\n",
    "                print(f\"   ğŸ“œ Ø³ÙƒØ±ÙˆÙ„ Ø±Ù‚Ù… {scroll_count}...\")\n",
    "\n",
    "                # ØªÙ†ÙÙŠØ° Ø§Ù„Ø³ÙƒØ±ÙˆÙ„ Ø¯Ø§Ø®Ù„ Ø§Ù„Ø­Ø§ÙˆÙŠØ©\n",
    "                if comments_container:\n",
    "                    driver.execute_script(\"arguments[0].scrollTop = arguments[0].scrollHeight;\", comments_container)\n",
    "                else:\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "                # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„ØªØ­Ù…ÙŠÙ„\n",
    "                time.sleep(4)\n",
    "\n",
    "                # Ø¬Ù…Ø¹ Ø§Ù„ÙƒÙˆÙ…Ù†ØªØ§Øª Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠØ© ÙÙ‚Ø· (Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ style Ùˆ line-height = 20px)\n",
    "                comment_spans = driver.find_elements(\n",
    "                    By.XPATH,\n",
    "                    \"//span[@dir='auto' and @style and contains(@style, '20px')]\"\n",
    "                )\n",
    "\n",
    "                # Ø­ÙØ¸ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ù‚Ø¨Ù„ Ø§Ù„Ø¬Ù…Ø¹ Ø§Ù„Ø¬Ø¯ÙŠØ¯\n",
    "                old_count = len(comments_set)\n",
    "\n",
    "                for span in comment_spans:\n",
    "                    text = span.text.strip()\n",
    "                    if text and len(text) > 0:\n",
    "                        # ØªØµÙÙŠØ© Ø¥Ø¶Ø§ÙÙŠØ©: ØªØ¬Ù†Ø¨ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù„ÙŠ ÙÙŠÙ‡Ø§ \"like\" Ø£Ùˆ \"Reply\"\n",
    "                        if not any(kw in text for kw in\n",
    "                                   [\"like\", \"Reply\", \"See translation\", \"View all\", \"Messages\", \"Home\", \"Search\",\n",
    "                                    \"Create\", \"Notifications\", \"Profile\", \"Also from Meta\", \"More\", \"Explore\",\n",
    "                                    \"Reels\"]):\n",
    "                            comments_set.add(text)\n",
    "\n",
    "                # Ø­Ø³Ø§Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø© Ø§Ù„Ù…Ø¶Ø§ÙØ©\n",
    "                new_count = len(comments_set)\n",
    "                new_comments_added = new_count - old_count\n",
    "\n",
    "                print(f\"   âœ… ØªÙ… Ø¬Ù…Ø¹ {new_comments_added} ØªØ¹Ù„ÙŠÙ‚ Ø¬Ø¯ÙŠØ¯ | Ø§Ù„Ø¥Ø¬Ù…Ø§Ù„ÙŠ: {new_count}\")\n",
    "\n",
    "                # ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ù†Ù‡Ø§ÙŠØ©\n",
    "                if comments_container:\n",
    "                    new_height = driver.execute_script(\"return arguments[0].scrollHeight;\", comments_container)\n",
    "                else:\n",
    "                    new_height = driver.execute_script(\"return document.body.scrollHeight;\")\n",
    "\n",
    "                # Ø¥Ø°Ø§ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ Ù„Ù… ÙŠØªØºÙŠØ±ØŒ Ø²ÙŠØ§Ø¯Ø© Ø§Ù„Ø¹Ø¯Ø§Ø¯\n",
    "                if new_height == last_height:\n",
    "                    same_height += 1\n",
    "                    print(f\"   âš ï¸ Ø§Ù„Ø§Ø±ØªÙØ§Ø¹ Ù„Ù… ÙŠØªØºÙŠØ± ({same_height}/{max_same})\")\n",
    "                else:\n",
    "                    same_height = 0  # Ø¥Ø¹Ø§Ø¯Ø© ØªØ¹ÙŠÙŠÙ† Ø§Ù„Ø¹Ø¯Ø§Ø¯\n",
    "\n",
    "                last_height = new_height\n",
    "\n",
    "            comments_list = list(comments_set)\n",
    "            num_comments = len(comments_list)\n",
    "            comments_joined = ' | '.join(comments_list) if comments_list else \"No comments\"\n",
    "            print(f\"âœ… ØªÙ… Ø¬Ù…Ø¹ {num_comments} ØªØ¹Ù„ÙŠÙ‚ Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø¨Ø¹Ø¯ {scroll_count} Ø¹Ù…Ù„ÙŠØ© Ø³ÙƒØ±ÙˆÙ„\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø¬Ù…Ø¹ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª: {e}\")\n",
    "            comments_joined = \"No comments\"\n",
    "            num_comments = 0\n",
    "\n",
    "        # --- Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø¥Ù„Ù‰ all_posts_data ---\n",
    "        all_posts_data[\"Comments\"].append(comments_joined)\n",
    "        all_posts_data[\"Comment Count\"].append(num_comments)\n",
    "\n",
    "        print(f\"\\nâœ… ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø§Ù„Ø¨ÙˆØ³Øª Ø±Ù‚Ù… {post_index}\")\n",
    "\n",
    "    # ======================================================================================================\n",
    "\n",
    "    # Ø¨Ø¹Ø¯ Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† ÙƒÙ„ Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"ğŸ‰ ØªÙ… Ø§Ù„Ø§Ù†ØªÙ‡Ø§Ø¡ Ù…Ù† Ø¬Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª ({len(posts)} Ø¨ÙˆØ³Øª)\")\n",
    "    print(f\"{'=' * 80}\\n\")\n",
    "\n",
    "    # --- Ø¥Ù†Ø´Ø§Ø¡ DataFrame ÙˆØ·Ø¨Ø§Ø¹ØªÙ‡ ---\n",
    "    df_all_posts = pd.DataFrame(all_posts_data)\n",
    "    print(\"\\nğŸ“Š ØªÙ… Ø¬Ù…Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª:\")\n",
    "    print(df_all_posts.to_string())\n",
    "\n",
    "    # --- Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª ---\n",
    "    all_posts_filename = f\"all_posts_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "    df_all_posts.to_csv(all_posts_filename, index=False, encoding='utf-8-sig')\n",
    "    print(f\"\\nğŸ’¾ ØªÙ… Ø­ÙØ¸ Ø¨ÙŠØ§Ù†Ø§Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨ÙˆØ³ØªØ§Øª ÙÙŠ: {all_posts_filename}\")\n",
    "    print(f\"âœ… Ø§Ù„Ù…Ù„Ù ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ {len(df_all_posts)} ØµÙ Ùˆ {len(df_all_posts.columns)} Ø¹Ù…ÙˆØ¯\")\n",
    "    print(f\"âœ… Ø¥Ø¬Ù…Ø§Ù„ÙŠ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø©: {df_all_posts['Comment Count'].sum()}\")\n",
    "else:\n",
    "    print(\"âŒ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ù†Ø´ÙˆØ±Ø§Øª Ù„Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc715ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36421dbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1e2d21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
